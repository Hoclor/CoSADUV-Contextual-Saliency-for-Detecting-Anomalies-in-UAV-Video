{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from random import randint\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100%\r"
     ]
    }
   ],
   "source": [
    "from util.data_utils import get_SALICON_datasets\n",
    "from util.data_utils import get_raw_SALICON_datasets\n",
    "\n",
    "train_data, val_data, test_data = get_SALICON_datasets('Dataset/Transformed') # 128x96\n",
    "#train_data, val_data, test_data = get_raw_SALICON_datasets(dataset_folder='/tmp/pbqk24_tmp') # 640x480\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n",
      "Loading weights for PlacesCNN_VGG16\n",
      "START TRAIN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/pytorch/0.4.0/torch/nn/functional.py:1749: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 0/5000] TRAIN loss: 0.000104\n",
      "[Iteration 50/5000] TRAIN loss: 0.000094\n",
      "[Iteration 100/5000] TRAIN loss: 0.000076\n",
      "[Iteration 150/5000] TRAIN loss: 0.000067\n",
      "[Iteration 200/5000] TRAIN loss: 0.000067\n",
      "[Iteration 250/5000] TRAIN loss: 0.000066\n",
      "[Iteration 300/5000] TRAIN loss: 0.000064\n",
      "[Iteration 350/5000] TRAIN loss: 0.000057\n",
      "[Iteration 400/5000] TRAIN loss: 0.000068\n",
      "[Iteration 450/5000] TRAIN loss: 0.000048\n",
      "[Epoch 0/10] TRAIN KLD Loss: 0.000068\n",
      "[Epoch 0/10] VAL KLD Loss: 0.000049\n",
      "[Iteration 500/5000] TRAIN loss: 0.000048\n",
      "[Iteration 550/5000] TRAIN loss: 0.000050\n",
      "[Iteration 600/5000] TRAIN loss: 0.000043\n",
      "[Iteration 650/5000] TRAIN loss: 0.000048\n",
      "[Iteration 700/5000] TRAIN loss: 0.000048\n",
      "[Iteration 750/5000] TRAIN loss: 0.000036\n",
      "[Iteration 800/5000] TRAIN loss: 0.000032\n",
      "[Iteration 850/5000] TRAIN loss: 0.000033\n",
      "[Iteration 900/5000] TRAIN loss: 0.000032\n",
      "[Iteration 950/5000] TRAIN loss: 0.000036\n",
      "[Epoch 1/10] TRAIN KLD Loss: 0.000029\n",
      "[Epoch 1/10] VAL KLD Loss: 0.000026\n",
      "[Iteration 1000/5000] TRAIN loss: 0.000032\n",
      "[Iteration 1050/5000] TRAIN loss: 0.000029\n",
      "[Iteration 1100/5000] TRAIN loss: 0.000026\n",
      "[Iteration 1150/5000] TRAIN loss: 0.000021\n",
      "[Iteration 1200/5000] TRAIN loss: 0.000029\n",
      "[Iteration 1250/5000] TRAIN loss: 0.000026\n",
      "[Iteration 1300/5000] TRAIN loss: 0.000025\n",
      "[Iteration 1350/5000] TRAIN loss: 0.000024\n",
      "[Iteration 1400/5000] TRAIN loss: 0.000024\n",
      "[Iteration 1450/5000] TRAIN loss: 0.000025\n",
      "[Epoch 2/10] TRAIN KLD Loss: 0.000028\n",
      "[Epoch 2/10] VAL KLD Loss: 0.000023\n",
      "[Iteration 1500/5000] TRAIN loss: 0.000020\n",
      "[Iteration 1550/5000] TRAIN loss: 0.000022\n",
      "[Iteration 1600/5000] TRAIN loss: 0.000018\n",
      "[Iteration 1650/5000] TRAIN loss: 0.000021\n",
      "[Iteration 1700/5000] TRAIN loss: 0.000020\n",
      "[Iteration 1750/5000] TRAIN loss: 0.000020\n",
      "[Iteration 1800/5000] TRAIN loss: 0.000020\n",
      "[Iteration 1850/5000] TRAIN loss: 0.000021\n",
      "[Iteration 1900/5000] TRAIN loss: 0.000020\n",
      "[Iteration 1950/5000] TRAIN loss: 0.000020\n",
      "[Epoch 3/10] TRAIN KLD Loss: 0.000022\n",
      "[Epoch 3/10] VAL KLD Loss: 0.000025\n",
      "[Iteration 2000/5000] TRAIN loss: 0.000019\n",
      "[Iteration 2050/5000] TRAIN loss: 0.000015\n",
      "[Iteration 2100/5000] TRAIN loss: 0.000017\n",
      "[Iteration 2150/5000] TRAIN loss: 0.000016\n",
      "[Iteration 2200/5000] TRAIN loss: 0.000017\n",
      "[Iteration 2250/5000] TRAIN loss: 0.000016\n",
      "[Iteration 2300/5000] TRAIN loss: 0.000018\n",
      "[Iteration 2350/5000] TRAIN loss: 0.000018\n",
      "[Iteration 2400/5000] TRAIN loss: 0.000017\n",
      "[Iteration 2450/5000] TRAIN loss: 0.000016\n",
      "[Epoch 4/10] TRAIN KLD Loss: 0.000017\n",
      "[Epoch 4/10] VAL KLD Loss: 0.000026\n",
      "[Iteration 2500/5000] TRAIN loss: 0.000015\n",
      "[Iteration 2550/5000] TRAIN loss: 0.000013\n",
      "[Iteration 2600/5000] TRAIN loss: 0.000011\n",
      "[Iteration 2650/5000] TRAIN loss: 0.000015\n",
      "[Iteration 2700/5000] TRAIN loss: 0.000014\n",
      "[Iteration 2750/5000] TRAIN loss: 0.000014\n",
      "[Iteration 2800/5000] TRAIN loss: 0.000014\n",
      "[Iteration 2850/5000] TRAIN loss: 0.000016\n",
      "[Iteration 2900/5000] TRAIN loss: 0.000014\n",
      "[Iteration 2950/5000] TRAIN loss: 0.000017\n",
      "[Epoch 5/10] TRAIN KLD Loss: 0.000015\n",
      "[Epoch 5/10] VAL KLD Loss: 0.000027\n",
      "[Iteration 3000/5000] TRAIN loss: 0.000013\n",
      "[Iteration 3050/5000] TRAIN loss: 0.000011\n",
      "[Iteration 3100/5000] TRAIN loss: 0.000012\n",
      "[Iteration 3150/5000] TRAIN loss: 0.000013\n",
      "[Iteration 3200/5000] TRAIN loss: 0.000014\n",
      "[Iteration 3250/5000] TRAIN loss: 0.000013\n"
     ]
    }
   ],
   "source": [
    "from util.data_utils import OverfitSampler\n",
    "from models.DSCLRCN_PyTorch2 import DSCLRCN\n",
    "from util.solver import Solver\n",
    "\n",
    "batchsize = 20 # Recommended: 20\n",
    "epoch_number = 10 # Recommended: 10 (epoch_number =~ batchsize/2)\n",
    "net_type = 'Seg' # 'Seg' or 'CNN' Recommended: Seg\n",
    "\n",
    "#num_train = 100\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batchsize, shuffle=True, num_workers=4)#,\n",
    "                                           #sampler=OverfitSampler(num_train))\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batchsize, shuffle=True, num_workers=4)\n",
    "\n",
    "# Attempt to train a model using the original image sizes\n",
    "model = DSCLRCN(input_dim=(96, 128), local_feats_net=net_type)\n",
    "solver = Solver(optim_args={'lr': 1e-4})\n",
    "solver.train(model, train_loader, val_loader, num_epochs=epoch_number, log_nth=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model:\n",
    "model.save('pretrained/model_{}_100_lr4_batch{}_epoch{}_2'.format(net_type, batchsize, epoch_number))\n",
    "with open('pretrained/solver_{}_100_lr4_batch{}_epoch{}_2.pkl'.format(net_type, batchsize, epoch_number), 'wb') as outf:\n",
    "    pickle.dump(solver, outf, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation loss over iterations:\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(solver.train_loss_history, 'o')\n",
    "plt.title('Train Loss')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(solver.val_loss_history, '-o')\n",
    "plt.title('Val Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loading some pretrained models to test them on the images:\n",
    "\n",
    "# model_cnn1 = torch.load('pretrained/model_cnn_100_lr4').cuda()\n",
    "# model_seg1 = torch.load('pretrained/model_seg_100_lr4').cuda()\n",
    "\n",
    "# model_cnn2 = torch.load('pretrained/model_cnn_noCon_100_lr4').cuda()\n",
    "# model_seg2 = torch.load('pretrained/model_seg_noCon_100_lr4').cuda()\n",
    "\n",
    "# model_cnn3 = torch.load('pretrained/model_cnn_noLSTM_100_lr4').cuda()\n",
    "# model_seg3 = torch.load('pretrained/model_seg_noLSTM_100_lr4').cuda()\n",
    "model = torch.load(\"pretrained/model_Seg_100_lr4_batch20_epoch10\", map_location='cpu')\n",
    "# Load the same model twice for now as we only have one trained model\n",
    "model_2 = torch.load(\"pretrained/model_Seg_100_lr4_batch20_epoch10\", map_location='cpu')\n",
    "#model_2 = torch.load(\"pretrained/model_Seg_100_lr4_batch10_epoch20\", map_location='cpu')\n",
    "\n",
    "# Move the models to the GPU if one is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    model_2 = model_2.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the original images from the test set ('test_datadict.pickle': dictionary of images and fixation maps)\n",
    "# This does NOT contain any fixation maps, as these are not provided with SALICON test images\n",
    "with open('Dataset/Transformed/test_datadict.pickle', 'rb') as f:\n",
    "        test_data_original = pickle.load(f)\n",
    "        print(\"Test data loaded\")\n",
    "\n",
    "# Loading the original images from the validation set ('val_datadict.pickle': dictionary of images and fixation maps)\n",
    "with open('Dataset/Transformed/val_datadict.pickle', 'rb') as f:\n",
    "        val_data_original = pickle.load(f)\n",
    "        print(\"Validation data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the different models on a random image from the val set:\n",
    "\n",
    "test_image_id = randint(0, len(test_data_original['images'])-1)\n",
    "val_image_id  = randint(0, len(val_data_original['images'])-1)\n",
    "\n",
    "x,y = test_data.__getitem__(test_image_id)\n",
    "x_val, y_val = val_data.__getitem__(val_image_id)\n",
    "\n",
    "original = test_data_original['images'][test_image_id]\n",
    "original_val = val_data_original['images'][val_image_id]\n",
    "\n",
    "x = x.contiguous().view(1, *x.size())\n",
    "x_2 = x[:]\n",
    "x_val = x_val.contiguous().view(1, *x_val.size())\n",
    "x_2_val = x_val[:]\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    x_val = x_val.cuda()\n",
    "    x_2 = x_2.cuda()\n",
    "    x_2_val = x_2_val.cuda()\n",
    "y = y.numpy()\n",
    "y_val = y_val.numpy()\n",
    "\n",
    "# model = torch.load('new_model').cuda()\n",
    "# model.eval()\n",
    "# model_cnn1.eval()\n",
    "\n",
    "# First model\n",
    "x_sal = model(Variable(x))\n",
    "if torch.cuda.is_available():\n",
    "    x_sal = x_sal.cpu()\n",
    "x_sal_nmp = x_sal.squeeze().data.numpy()\n",
    "# Blur the saliency map\n",
    "sigma = 0.035*min(x_sal_nmp.shape)\n",
    "x_sal_nmp = cv2.GaussianBlur(x_sal_nmp, (int(4*sigma), int(4*sigma)), sigma)\n",
    "\n",
    "x_val_sal = model(Variable(x_val))\n",
    "if torch.cuda.is_available():\n",
    "    x_val_sal = x_val_sal.cpu()\n",
    "x_val_sal_nmp = x_val_sal.squeeze().data.numpy()\n",
    "# Blur the saliency map\n",
    "x_val_sal_nmp = cv2.GaussianBlur(x_val_sal_nmp, (int(4*sigma), int(4*sigma)), sigma)\n",
    "\n",
    "# Second model\n",
    "x_2_sal = model_2(Variable(x_2))\n",
    "if torch.cuda.is_available():\n",
    "    x_2_sal = x_2_sal.cpu()\n",
    "x_2_sal_nmp = x_2_sal.squeeze().data.numpy()\n",
    "# Blur the saliency map\n",
    "x_2_sal_nmp = cv2.GaussianBlur(x_2_sal_nmp, (int(4*sigma), int(4*sigma)), sigma)\n",
    "\n",
    "\n",
    "x_2_val_sal = model(Variable(x_2_val))\n",
    "if torch.cuda.is_available():\n",
    "    x_2_val_sal = x_2_val_sal.cpu()\n",
    "x_2_val_sal_nmp = x_2_val_sal.squeeze().data.numpy()\n",
    "# Blur the saliency map\n",
    "x_2_val_sal_nmp = cv2.GaussianBlur(x_2_val_sal_nmp, (int(4*sigma), int(4*sigma)), sigma)\n",
    "\n",
    "\n",
    "# Plot the output\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(2,4,1); plt.title('Original')\n",
    "plt.imshow(original)\n",
    "plt.subplot(2,4,2); plt.title('Ground Truth')\n",
    "plt.imshow(y, cmap='gray')\n",
    "\n",
    "plt.subplot(2,4,3)\n",
    "# plt.imshow(x_sal_nmp1, cmap='gray'); plt.title('CNN+Context+LSTM')\n",
    "plt.imshow(x_sal_nmp, cmap='gray'); plt.title('Model b20 e10')\n",
    "# Second model\n",
    "plt.subplot(2,4,4)\n",
    "plt.imshow(x_2_sal_nmp, cmap='gray'); plt.title('Model b10 e20')\n",
    "\n",
    "plt.subplot(2,4,5); plt.title('Original Val')\n",
    "plt.imshow(original_val)\n",
    "plt.subplot(2,4,6); plt.title('Ground Truth Val')\n",
    "plt.imshow(y_val, cmap='gray')\n",
    "\n",
    "plt.subplot(2,4,7)\n",
    "plt.imshow(x_val_sal_nmp, cmap='gray'); plt.title('Model b20 e10')\n",
    "# Second model\n",
    "plt.subplot(2,4,8)\n",
    "plt.imshow(x_2_val_sal_nmp, cmap='gray'); plt.title('Model b10 e20')\n",
    "\n",
    "\n",
    "# plt.subplot(2,4,6)\n",
    "# plt.imshow(x_sal_nmp2, cmap='gray'); plt.title('Seg.+Context+LSTM')\n",
    "\n",
    "# plt.subplot(2,4,3)\n",
    "# plt.imshow(x_sal_nmp3, cmap='gray'); plt.title('CNN+LSTM')\n",
    "# plt.subplot(2,4,7)\n",
    "# plt.imshow(x_sal_nmp4, cmap='gray'); plt.title('Seg.+LSTM')\n",
    "\n",
    "# plt.subplot(2,4,4)\n",
    "# plt.imshow(x_sal_nmp5, cmap='gray'); plt.title('CNN')\n",
    "\n",
    "# plt.subplot(2,4,8)\n",
    "# plt.imshow(x_sal_nmp6, cmap='gray'); plt.title('Seg.')\n",
    "# plt.savefig('ResExamples/example_'+str(test_image_id)+'.png')\n",
    "plt.show()\n",
    "\n",
    "# model.eval()\n",
    "# x_saln = model(Variable(x))\n",
    "# x_saln_nmp = x_saln.squeeze().cpu().data.numpy()\n",
    "\n",
    "# plt.imshow(x_saln_nmp, cmap='gray'); plt.title('new')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pearson Cross Correlation loss function\n",
    "def PCCLoss_torch(x, y):\n",
    "    \"\"\"Computes Pearson Cross Correlation loss\n",
    "    :param x: prediction\n",
    "    :param y: label\n",
    "    \"\"\"\n",
    "    vx = x - torch.mean(x)\n",
    "    vy = y - torch.mean(y)\n",
    "    \n",
    "    loss = torch.sum(vx*y) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "    return loss\n",
    "\n",
    "def PCCLoss_numpy(x, y):\n",
    "    \"\"\"Computes Pearson Cross Correlation loss\n",
    "    :param x: prediction\n",
    "    :param y: label\n",
    "    \"\"\"\n",
    "    vx = x - np.mean(x)\n",
    "    vy = y - np.mean(y)\n",
    "    \n",
    "    loss = np.sum(vx*y) / (np.sqrt(np.sum(vx ** 2)) * np.sqrt(np.sum(vy ** 2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO keep working here, strange divergence of results between using torch pipeline and numpy pipeline\n",
    "mean_image = np.load('Dataset/Transformed/mean_image.npy').astype(np.float32)/255.\n",
    "# Define a function for testing a model\n",
    "def test_model(model, data_source, loss_fn=PCCLoss_numpy, input_size=(640, 480)):\n",
    "    test_loader = torch.utils.data.DataLoader(data_source, batch_size=5, shuffle=True, num_workers=4)\n",
    "    mean_image_batch = np.repeat(mean_image, 5, axis=0)\n",
    "    testLosses = []\n",
    "    \n",
    "    for data in tqdm(test_loader):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = Variable(inputs.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "        ### TORCH PIPELINE ###\n",
    "        if loss_fn == PCCLoss_torch:\n",
    "            outputs = model(inputs).squeeze()\n",
    "\n",
    "            # Divides each value in each fixation map by the sum of all values in that fixation map\n",
    "            labels_sum = torch.sum(labels.contiguous().view(labels.size(0),-1), dim=1)\n",
    "            labels /= labels_sum.contiguous().view(*labels_sum.size(), 1, 1).expand_as(labels)\n",
    "            \n",
    "#             cv2.imshow('Output', outputs.cpu().data.numpy()[0, :, :])\n",
    "#             cv2.imshow('Label', labels.cpu().numpy()[0, :, :])\n",
    "#             cv2.waitKey(0)\n",
    "        \n",
    "        ### NUMPY PIPELINE ###\n",
    "        if loss_fn == PCCLoss_numpy:\n",
    "            # Problem: model yields outputs that are very low (zero-centered?) pixel values.\n",
    "            outputs = model(inputs).squeeze()\n",
    "            outputs = outputs.cpu().data.numpy()\n",
    "            # Try to process the output with the mean image or this image's mean or something similar\n",
    "            #outputs = np.add(outputs, mean_image_batch)\n",
    "            \n",
    "#             cv2.imshow('Output pre', outputs[0, :, :])\n",
    "#             cv2.waitKey(0)\n",
    "            \n",
    "            # Resize the images to input size\n",
    "            outputs = np.array([cv2.resize(output, input_size) for output in outputs])\n",
    "            \n",
    "            # Apply a Gaussian filter to blur the saliency maps\n",
    "            sigma = 0.035*min(input_size[0], input_size[1])\n",
    "            outputs = np.array([cv2.GaussianBlur(output, (int(4*sigma), int(4*sigma)), sigma) for output in outputs])\n",
    "\n",
    "            # Divides each value in each fixation map by the sum of all values in that fixation map\n",
    "            labels_sum = torch.sum(labels.contiguous().view(labels.size(0),-1), dim=1)\n",
    "            labels /= labels_sum.contiguous().view(*labels_sum.size(), 1, 1).expand_as(labels)\n",
    "            \n",
    "            labels = labels.cpu().numpy()\n",
    "            \n",
    "#             print(outputs.shape)\n",
    "#             temp = np.hstack((outputs[0, :, :], labels[0, :, :]))\n",
    "#             cv2.imshow('Output and label', temp)\n",
    "#             cv2.imshow('Output', outputs[0, :, :])\n",
    "#             cv2.imshow('Label', labels[0, :, :])\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "        ### BOTH PIPELINES ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        testLosses.append(loss_fn(outputs, labels).item())\n",
    "    \n",
    "    return testLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining PCC Loss values on the test set for different models:\n",
    "\n",
    "# loss_fn = nn.KLDivLoss()\n",
    "# Use Pearson Cross Correlation loss\n",
    "loss_fn = PCCLoss_numpy\n",
    "\n",
    "# test on validation data, as we don't have ground truths for the test data (this was also done in original DSCLRCN paper)\n",
    "# test_loader = torch.utils.data.DataLoader(val_data, batch_size=5, shuffle=True, num_workers=4)\n",
    "\n",
    "TestLosses_CNN1 = []\n",
    "TestLosses_Seg1 = []\n",
    "TestLosses_CNN2 = []\n",
    "TestLosses_Seg2 = []\n",
    "TestLosses_CNN3 = []\n",
    "TestLosses_Seg3 = []\n",
    "# for i, data in enumerate(tqdm(test_loader)):\n",
    "#     inputs, labels = data\n",
    "#     if torch.cuda.is_available():\n",
    "#         inputs = Variable(inputs.cuda())\n",
    "#         labels = Variable(labels.cuda())\n",
    "#     else:\n",
    "#         inputs = Variable(inputs)\n",
    "#         labels = Variable(labels)\n",
    "    \n",
    "#     #test_outputs_CNN1 = model_cnn1(inputs).squeeze()\n",
    "#     test_outputs_Seg1 = model(inputs).squeeze()\n",
    "    \n",
    "#     #test_outputs_CNN2 = model_cnn2(inputs).squeeze()\n",
    "#     test_outputs_Seg2 = model_2(inputs).squeeze()\n",
    "    \n",
    "#     #test_outputs_CNN3 = model_cnn3(inputs).squeeze()\n",
    "#     #test_outputs_Seg3 = model_seg3(inputs).squeeze()\n",
    "    \n",
    "#     labels_sum = torch.sum(labels.contiguous().view(labels.size(0),-1), dim=1)\n",
    "#     labels /= labels_sum.contiguous().view(*labels_sum.size(), 1, 1).expand_as(labels)\n",
    "\n",
    "#     #TestLosses_CNN1.append(loss_fn(torch.log(test_outputs_CNN1), labels).data[0])\n",
    "#     TestLosses_Seg1.append(loss_fn(test_outputs_Seg1, labels).data[0])\n",
    "    \n",
    "#     #TestLosses_CNN2.append(loss_fn(torch.log(test_outputs_CNN2), labels).data[0])\n",
    "#     TestLosses_Seg2.append(loss_fn(test_outputs_Seg2, labels).data[0])\n",
    "    \n",
    "#     #TestLosses_CNN3.append(loss_fn(torch.log(test_outputs_CNN3), labels).data[0])\n",
    "#     #TestLosses_Seg3.append(loss_fn(torch.log(test_outputs_Seg3), labels).data[0])\n",
    "\n",
    "TestLosses_Seg1 = test_model(model, val_data, loss_fn=loss_fn, input_size=(128, 96))\n",
    "\n",
    "TestLosses_Seg2 = test_model(model_2, val_data, loss_fn=PCCLoss_torch, input_size=(128, 96))\n",
    "\n",
    "print()\n",
    "#print('TestLoss (CNN): ', np.mean(TestLosses_CNN1), np.mean(TestLosses_CNN2), np.mean(TestLosses_CNN3))\n",
    "print('Pearson Cross Correlation Loss on Validation set (Seg):\\nModel 1:', np.mean(TestLosses_Seg1), \"\\nModel 2:\" , np.mean(TestLosses_Seg2))#, \"\\nModel 3:\", np.mean(TestLosses_Seg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
