{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from random import randint\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading dataset from C:/Users/simon/Downloads/Project Datasets/SALICON/\n",
      "\n",
      "Reading image files: All\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20000/20000 [03:00<00:00, 110.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading fixation maps: Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:26<00:00, 377.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading fixation maps: Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:11<00:00, 430.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean image\n",
      "Progress: 100%\r"
     ]
    }
   ],
   "source": [
    "from util.data_utils import get_SALICON_datasets\n",
    "from util.data_utils import get_raw_SALICON_datasets\n",
    "\n",
    "#train_data, val_data, test_data = get_SALICON_datasets('Dataset/Transformed')\n",
    "train_data, val_data, test_data = get_raw_SALICON_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n",
      "Loading weights for PlacesCNN_VGG16\n",
      "START TRAIN.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cf81f29afb1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDSCLRCN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m640\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_feats_net\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptim_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_nth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\GitRepositories\\MastersProject\\DSCLRCN-PyTorch\\util\\solver.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model, train_loader, val_loader, num_epochs, log_nth)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;31m# Batch of items in training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0miter_per_epoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MastersProject\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MastersProject\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m  \u001b[1;31m# ensure that the worker exits on process exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0m_update_worker_pids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MastersProject\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MastersProject\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MastersProject\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MastersProject\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MastersProject\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from util.data_utils import OverfitSampler\n",
    "from models.DSCLRCN_PyTorch2 import DSCLRCN\n",
    "from util.solver import Solver\n",
    "\n",
    "batchsize = 20 # Recommended: 20\n",
    "epoch_number = 10 # Recommended: 10 (epoch_number =~ batchsize/2)\n",
    "net_type = 'Seg' # 'Seg' or 'CNN' Recommended: Seg\n",
    "\n",
    "#num_train = 100\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batchsize, shuffle=True, num_workers=4)#,\n",
    "                                           #sampler=OverfitSampler(num_train))\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batchsize, shuffle=True, num_workers=4)\n",
    "\n",
    "# Attempt to train a model using the original image sizes\n",
    "model = DSCLRCN(input_dim=(480, 640), local_feats_net=net_type)\n",
    "solver = Solver(optim_args={'lr': 1e-4})\n",
    "solver.train(model, train_loader, val_loader, num_epochs=epoch_number, log_nth=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model:\n",
    "model.save('pretrained/model_{}_100_lr4_batch{}_epoch{}'.format(net_type, batchsize, epoch_number))\n",
    "with open('pretrained/solver_{}_100_lr4_batch{}_epoch{}.pkl'.format(net_type, batchsize, epoch_number), 'wb') as outf:\n",
    "    pickle.dump(solver, outf, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation loss over iterations:\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(solver.train_loss_history, 'o')\n",
    "plt.title('Train Loss')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(solver.val_loss_history, '-o')\n",
    "plt.title('Val Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loading some pretrained models to test them on the images:\n",
    "\n",
    "# model_cnn1 = torch.load('pretrained/model_cnn_100_lr4').cuda()\n",
    "# model_seg1 = torch.load('pretrained/model_seg_100_lr4').cuda()\n",
    "\n",
    "# model_cnn2 = torch.load('pretrained/model_cnn_noCon_100_lr4').cuda()\n",
    "# model_seg2 = torch.load('pretrained/model_seg_noCon_100_lr4').cuda()\n",
    "\n",
    "# model_cnn3 = torch.load('pretrained/model_cnn_noLSTM_100_lr4').cuda()\n",
    "# model_seg3 = torch.load('pretrained/model_seg_noLSTM_100_lr4').cuda()\n",
    "model = torch.load(\"pretrained/model_Seg_100_lr4_batch20_epoch10\", map_location='cpu')\n",
    "model_2 = torch.load(\"pretrained/model_Seg_100_lr4_batch10_epoch20\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the original images from the test set ('test_datadict.pickle': dictionary of images and fixation maps)\n",
    "# This does NOT contain any fixation maps, as these are not provided with SALICON test images\n",
    "with open('Dataset/Transformed/test_datadict.pickle', 'rb') as f:\n",
    "        test_data_original = pickle.load(f)\n",
    "        print(\"Test data loaded\")\n",
    "\n",
    "# Loading the original images from the validation set ('val_datadict.pickle': dictionary of images and fixation maps)\n",
    "with open('Dataset/Transformed/val_datadict.pickle', 'rb') as f:\n",
    "        val_data_original = pickle.load(f)\n",
    "        print(\"Validation data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the different models on a random image from the val set:\n",
    "\n",
    "test_image_id = randint(0, len(test_data_original['images'])-1)\n",
    "val_image_id  = randint(0, len(val_data_original['images'])-1)\n",
    "\n",
    "x,y = test_data.__getitem__(test_image_id)\n",
    "x_val, y_val = val_data.__getitem__(val_image_id)\n",
    "\n",
    "original = test_data_original['images'][test_image_id]\n",
    "original_val = val_data_original['images'][val_image_id]\n",
    "\n",
    "x = x.contiguous().view(1, *x.size())\n",
    "x_2 = x[:]\n",
    "x_val = x_val.contiguous().view(1, *x_val.size())\n",
    "x_2_val = x_val[:]\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    x_val = x_val.cuda()\n",
    "y = y.numpy()\n",
    "y_val = y_val.numpy()\n",
    "\n",
    "# model = torch.load('new_model').cuda()\n",
    "# model.eval()\n",
    "# model_cnn1.eval()\n",
    "\n",
    "# First model\n",
    "x_sal = model(Variable(x))\n",
    "x_sal_nmp = x_sal.squeeze().data.numpy()\n",
    "# Blur the saliency map\n",
    "sigma = 0.035*min(128, 96)\n",
    "x_sal_nmp = cv2.GaussianBlur(x_sal_nmp, (int(4*sigma), int(4*sigma)), sigma)\n",
    "\n",
    "x_val_sal = model(Variable(x_val))\n",
    "x_val_sal_nmp = x_val_sal.squeeze().data.numpy()\n",
    "# Blur the saliency map\n",
    "sigma = 0.035*min(128, 96)\n",
    "x_val_sal_nmp = cv2.GaussianBlur(x_val_sal_nmp, (int(4*sigma), int(4*sigma)), sigma)\n",
    "\n",
    "# Second model\n",
    "x_2_sal = model_2(Variable(x_2))\n",
    "x_2_sal_nmp = x_2_sal.squeeze().data.numpy()\n",
    "# Blur the saliency map\n",
    "sigma = 0.035*min(128, 96)\n",
    "x_2_sal_nmp = cv2.GaussianBlur(x_2_sal_nmp, (int(4*sigma), int(4*sigma)), sigma)\n",
    "\n",
    "\n",
    "x_2_val_sal = model(Variable(x_2_val))\n",
    "x_2_val_sal_nmp = x_2_val_sal.squeeze().data.numpy()\n",
    "# Blur the saliency map\n",
    "sigma = 0.035*min(128, 96)\n",
    "x_2_val_sal_nmp = cv2.GaussianBlur(x_2_val_sal_nmp, (int(4*sigma), int(4*sigma)), sigma)\n",
    "\n",
    "\n",
    "# Plot the output\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(2,4,1); plt.title('Original')\n",
    "plt.imshow(original)\n",
    "plt.subplot(2,4,2); plt.title('Ground Truth')\n",
    "plt.imshow(y, cmap='gray')\n",
    "\n",
    "plt.subplot(2,4,3)\n",
    "# plt.imshow(x_sal_nmp1, cmap='gray'); plt.title('CNN+Context+LSTM')\n",
    "plt.imshow(x_sal_nmp, cmap='gray'); plt.title('Model b20 e10')\n",
    "# Second model\n",
    "plt.subplot(2,4,4)\n",
    "plt.imshow(x_2_sal_nmp, cmap='gray'); plt.title('Model b10 e20')\n",
    "\n",
    "plt.subplot(2,4,5); plt.title('Original Val')\n",
    "plt.imshow(original_val)\n",
    "plt.subplot(2,4,6); plt.title('Ground Truth Val')\n",
    "plt.imshow(y_val, cmap='gray')\n",
    "\n",
    "plt.subplot(2,4,7)\n",
    "plt.imshow(x_val_sal_nmp, cmap='gray'); plt.title('Model b20 e10')\n",
    "# Second model\n",
    "plt.subplot(2,4,8)\n",
    "plt.imshow(x_2_val_sal_nmp, cmap='gray'); plt.title('Model b10 e20')\n",
    "\n",
    "\n",
    "# plt.subplot(2,4,6)\n",
    "# plt.imshow(x_sal_nmp2, cmap='gray'); plt.title('Seg.+Context+LSTM')\n",
    "\n",
    "# plt.subplot(2,4,3)\n",
    "# plt.imshow(x_sal_nmp3, cmap='gray'); plt.title('CNN+LSTM')\n",
    "# plt.subplot(2,4,7)\n",
    "# plt.imshow(x_sal_nmp4, cmap='gray'); plt.title('Seg.+LSTM')\n",
    "\n",
    "# plt.subplot(2,4,4)\n",
    "# plt.imshow(x_sal_nmp5, cmap='gray'); plt.title('CNN')\n",
    "\n",
    "# plt.subplot(2,4,8)\n",
    "# plt.imshow(x_sal_nmp6, cmap='gray'); plt.title('Seg.')\n",
    "# plt.savefig('ResExamples/example_'+str(test_image_id)+'.png')\n",
    "plt.show()\n",
    "\n",
    "# model.eval()\n",
    "# x_saln = model(Variable(x))\n",
    "# x_saln_nmp = x_saln.squeeze().cpu().data.numpy()\n",
    "\n",
    "# plt.imshow(x_saln_nmp, cmap='gray'); plt.title('new')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pearson Cross Correlation loss function\n",
    "def PCCLoss(x, y):\n",
    "    \"\"\"Computes Pearson Cross Correlation loss\n",
    "    :param x: prediction\n",
    "    :param y: label\n",
    "    \"\"\"\n",
    "\n",
    "    vx = x - torch.mean(x)\n",
    "    vy = y - torch.mean(y)\n",
    "    \n",
    "    loss = torch.sum(vx*y) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for testing a model\n",
    "def test_model(model, data_source, loss_fn=PCCLoss, input_size=(480, 640)):\n",
    "    test_loader = torch.utils.data.DataLoader(data_source, batch_size=5, shuffle=True, num_workers=4)\n",
    "    testLosses = []\n",
    "    \n",
    "    for data in tqdm(test_loader):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = Variable(inputs.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs = Variable(inputs)\n",
    "            lables = Variable(labels)\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        \n",
    "        # Resize the images to input size\n",
    "        outputs = output.reshape((-1, input_size[0], input_size[1], output.shape[3]))\n",
    "        \n",
    "        # Apply a Gaussian filter to blur the saliency maps\n",
    "        sigma = 0.035*min(input_size[0], input_size[1])\n",
    "        outputs = np.array([cv2.GaussianBlur(output, (int(4*sigma), int(4*sigma)), sigma) for output in outputs])\n",
    "        \n",
    "        labels_sum = torch.sum(labels.contiguous().view(labels.size(0),-1), dim=1)\n",
    "        labels /= labels_sum.contiguous().view(*labels_sum.size(), 1, 1).expand_as(labels)\n",
    "\n",
    "        testLosses.append(loss_fn(outputs, labels).data[0])\n",
    "    \n",
    "    return testLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining PCC Loss values on the test set for different models:\n",
    "\n",
    "# loss_fn = nn.KLDivLoss()\n",
    "# Use Pearson Cross Correlation loss\n",
    "loss_fn = PCCLoss\n",
    "\n",
    "# test on validation data, as we don't have ground truths for the test data (this was also done in original DSCLRCN paper)\n",
    "# test_loader = torch.utils.data.DataLoader(val_data, batch_size=5, shuffle=True, num_workers=4)\n",
    "\n",
    "TestLosses_CNN1 = []\n",
    "TestLosses_Seg1 = []\n",
    "TestLosses_CNN2 = []\n",
    "TestLosses_Seg2 = []\n",
    "TestLosses_CNN3 = []\n",
    "TestLosses_Seg3 = []\n",
    "# for i, data in enumerate(tqdm(test_loader)):\n",
    "#     inputs, labels = data\n",
    "#     if torch.cuda.is_available():\n",
    "#         inputs = Variable(inputs.cuda())\n",
    "#         labels = Variable(labels.cuda())\n",
    "#     else:\n",
    "#         inputs = Variable(inputs)\n",
    "#         labels = Variable(labels)\n",
    "    \n",
    "#     #test_outputs_CNN1 = model_cnn1(inputs).squeeze()\n",
    "#     test_outputs_Seg1 = model(inputs).squeeze()\n",
    "    \n",
    "#     #test_outputs_CNN2 = model_cnn2(inputs).squeeze()\n",
    "#     test_outputs_Seg2 = model_2(inputs).squeeze()\n",
    "    \n",
    "#     #test_outputs_CNN3 = model_cnn3(inputs).squeeze()\n",
    "#     #test_outputs_Seg3 = model_seg3(inputs).squeeze()\n",
    "    \n",
    "#     labels_sum = torch.sum(labels.contiguous().view(labels.size(0),-1), dim=1)\n",
    "#     labels /= labels_sum.contiguous().view(*labels_sum.size(), 1, 1).expand_as(labels)\n",
    "\n",
    "#     #TestLosses_CNN1.append(loss_fn(torch.log(test_outputs_CNN1), labels).data[0])\n",
    "#     TestLosses_Seg1.append(loss_fn(test_outputs_Seg1, labels).data[0])\n",
    "    \n",
    "#     #TestLosses_CNN2.append(loss_fn(torch.log(test_outputs_CNN2), labels).data[0])\n",
    "#     TestLosses_Seg2.append(loss_fn(test_outputs_Seg2, labels).data[0])\n",
    "    \n",
    "#     #TestLosses_CNN3.append(loss_fn(torch.log(test_outputs_CNN3), labels).data[0])\n",
    "#     #TestLosses_Seg3.append(loss_fn(torch.log(test_outputs_Seg3), labels).data[0])\n",
    "\n",
    "TestLosses_Seg1 = test_model(model, val_data, loss_fn=loss_fn)\n",
    "\n",
    "TestLosses_Seg2 = test_model(model_2, val_data, loss_fn=loss_fn)\n",
    "\n",
    "print()\n",
    "#print('TestLoss (CNN): ', np.mean(TestLosses_CNN1), np.mean(TestLosses_CNN2), np.mean(TestLosses_CNN3))\n",
    "print('Pearson Cross Correlation Loss on Validation set (Seg):\\nModel 1:', np.mean(TestLosses_Seg1), \"\\nModel 2:\" , np.mean(TestLosses_Seg2))#, \"\\nModel 3:\", np.mean(TestLosses_Seg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
